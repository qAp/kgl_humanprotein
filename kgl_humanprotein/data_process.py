# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_data_process.ipynb (unless otherwise specified).

__all__ = ['imgids_from_directory', 'imgids_testing', 'read_img', 'load_RGBY_image', 'save_image', 'CellSegmentator',
           'load_segmentator', 'get_cellmask', 'encode_binary_mask', 'coco_rle_encode', 'rle_encode', 'rle_decode',
           'mask2rles', 'rles2bboxes', 'segment_image', 'segment_images', 'resize_image', 'crop_image',
           'remove_faint_greens', 'pad_to_square', 'load_seg_trn', 'split_cells', 'generate_crops', 'fill_targets',
           'generate_meta', 'get_meta', 'create_split_file', 'create_random_split', 'load_match_info',
           'generate_noleak_split', 'get_img_mean_std']

# Cell
import os
import ast
from pathlib import Path
from itertools import groupby
import functools
import mlcrate
from multiprocessing import Pool
from pycocotools import mask as mutils
from pycocotools import _mask as coco_mask
import numpy as np
import pandas as pd
import cv2, PIL
import zlib
import base64
import zipfile
import uuid
from sklearn.preprocessing import LabelEncoder

from .config.config import *
from .utils.common_util import *

# Cell
def imgids_from_directory(path):
    if isinstance(path, str):
        path = Path(path)

    imgids = set(n.stem.split('_')[0] for n in path.iterdir())
    return list(imgids)

# Cell
imgids_testing = [
    '000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0',
    '001838f8-bbca-11e8-b2bc-ac1f6b6435d0',
    '000c99ba-bba4-11e8-b2b9-ac1f6b6435d0',
    'a34d8680-bb99-11e8-b2b9-ac1f6b6435d0',
    '000a9596-bbc4-11e8-b2bc-ac1f6b6435d0']

# Cell

def read_img(dir_data, image_id, color, image_size=None, suffix='.png'):
    filename = dir_data/f'{image_id}_{color}{suffix}'
    assert filename.exists(), f'not found {filename}'

    img = cv2.imread(str(filename), cv2.IMREAD_UNCHANGED)

    if image_size is not None:
        img = cv2.resize(img, (image_size, image_size))

    if img.max() > 255:
        img_max = img.max()
        img = (img/255).astype('uint8')

    return img



def load_RGBY_image(dir_data, image_id,
                    rgb_only=False, suffix='.png', image_size=None):

    red, green, blue = [
        read_img(dir_data, image_id, color, image_size, suffix)
        for color in ('red', 'green', 'blue')]

    channels = [red, green, blue]

    if not rgb_only:
        yellow = read_img(
            dir_data, image_id, "yellow", image_size, suffix)
        channels.append(yellow)

    stacked_images = np.transpose(np.array(channels), (1, 2, 0))

    return stacked_images

# Cell

def save_image(dst, imgid, img):
    dst = Path(dst)
    for ch, color in enumerate(['red', 'green', 'blue', 'yellow']):
        cv2.imwrite(str(dst / f'{imgid}_{color}.png'), img[..., ch])


# Cell

import hpacellseg.cellsegmentator as cellsegmentator
from hpacellseg.utils import label_cell, label_nuclei
from tqdm import tqdm

class CellSegmentator(cellsegmentator.CellSegmentator):
    def __init__(self, nuc_model, cell_model, *args, **kwargs):
        nuc_model = str(nuc_model)
        cell_model = str(cell_model)
        super().__init__(nuc_model, cell_model, *args, **kwargs)

    def __call__(self, red, yellow, blue):
        '''
        `red`: list
          Red images' file paths.
        `yellow`: list
          Yellow images' file paths.
        `blue`: list
          Blue images' file paths.
        '''
        assert len(red) == len(yellow) == len(blue)

        if isinstance(red[0], Path):
            red, yellow, blue = (
                [str(n) for n in fns]
                for fns in [red, yellow, blue])

        segs_nucl = self.pred_nuclei(blue)
        segs_cell = self.pred_cells([red, yellow, blue])

        masks = []
        for seg_nucl, seg_cell in zip(segs_nucl, segs_cell):
            mask_nucl, mask_cell = label_cell(seg_nucl, seg_cell)
            masks.append((mask_nucl, mask_cell))

        return masks



def load_segmentator(
    dir_segmentator_models, scale_factor=0.25, device="cuda",
    padding=True, multi_channel_model=True):

    model_nucl = dir_segmentator_models / 'nuclei-model.pth'
    model_cell = dir_segmentator_models / 'cell-model.pth'

    segmentator = CellSegmentator(
        model_nucl, model_cell,
        scale_factor=scale_factor, device=device, padding=padding,
        multi_channel_model=multi_channel_model)

    return segmentator



def get_cellmask(img, segmentator):
    img_r, img_y, img_b = img[...,0], img[...,3], img[...,2]

    masks = segmentator(red=[img_r], yellow=[img_y], blue=[img_b])

    _, mask = masks[0]
    return mask

# Cell

def encode_binary_mask(mask):
    """Converts a binary mask into OID challenge encoding ascii text."""

    # check input mask --
    if mask.dtype != np.bool:
        raise ValueError(
        "encode_binary_mask expects a binary mask, received dtype == %s" %
        mask.dtype)

    mask = np.squeeze(mask)
    if len(mask.shape) != 2:
        raise ValueError(
        "encode_binary_mask expects a 2d mask, received shape == %s" %
        mask.shape)

    # convert input mask to expected COCO API input --
    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)
    mask_to_encode = mask_to_encode.astype(np.uint8)
    mask_to_encode = np.asfortranarray(mask_to_encode)

    # RLE encode mask --
    encoded_mask = coco_mask.encode(mask_to_encode)[0]["counts"]

    # compress and base64 encoding --
    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
    base64_str = base64.b64encode(binary_str)
    return base64_str.decode()


def coco_rle_encode(bmask):
    rle = {'counts': [], 'size': list(bmask.shape)}
    counts = rle.get('counts')
    for i, (value, elements) in enumerate(groupby(bmask.ravel(order='F'))):
        if i == 0 and value == 1:
            counts.append(0)
        counts.append(len(list(elements)))
    return rle

# Cell

def rle_encode(img, mask_val=1):
    """
    Turns our masks into RLE encoding to easily store them
    and feed them into models later on
    https://en.wikipedia.org/wiki/Run-length_encoding

    Args:
        img (np.array): Segmentation array
        mask_val (int): Which value to use to create the RLE

    Returns:
        RLE string

    """
    dots = np.where(img.T.flatten() == mask_val)[0]
    run_lengths = []
    prev = -2
    for b in dots:
        if (b>prev+1): run_lengths.extend((b + 1, 0))
        run_lengths[-1] += 1
        prev = b

    return ' '.join([str(x) for x in run_lengths])


def rle_decode(rle_string, height, width):
    """ Convert RLE sttring into a binary mask

    Args:
        rle_string (rle_string): Run length encoding containing
            segmentation mask information
        height (int): Height of the original image the map comes from
        width (int): Width of the original image the map comes from

    Returns:
        Numpy array of the binary segmentation mask for a given cell
    """
    rows,cols = height,width
    rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]
    rle_pairs = np.array(rle_numbers).reshape(-1,2)
    img = np.zeros(rows*cols,dtype=np.uint8)
    for index,length in rle_pairs:
        index -= 1
        img[index:index+length] = 255
    img = img.reshape(cols,rows)
    img = img.T
    img = (img / 255).astype(np.uint8)
    return img


# Cell

def mask2rles(mask):
    '''
    Args:
        mask (np.array): 2-D array with discrete values each
            representing a different class or object.
        rles (list): COCO run-length encoding:
            {'size': [height, width],
             'counts': encoded RLE}
    '''
    ids_cell = np.unique(mask)

    rles = []
    for id in ids_cell:
        if id == 0:
            continue

        bmask = np.where(mask == id, 1, 0)
        bmask = np.asfortranarray(bmask).astype(np.uint8)
        rle = mutils.encode(bmask)
        rles.append(rle)

    return rles

# Cell
def rles2bboxes(rles):
    if len(rles) == 0:
        return []

    bboxes = mutils.toBbox(rles)
    bboxes[:,2] += bboxes[:,0]
    bboxes[:,3] += bboxes[:,1]

    return bboxes

# Cell

def segment_image(dir_img=None, imgid=None, segmentator=None):
    img = load_RGBY_image(dir_img, imgid)
    mask = get_cellmask(img, segmentator)
    rles = mask2rles(mask)
    bboxes = rles2bboxes(rles)

    ids = [f'{imgid}_{i}' for i in range(len(rles))]
    df = pd.DataFrame(
        {'Id': ids, 'rle': rles, 'bbox': list(bboxes)})

    return df



def segment_images(dir_img, imgids, segmentator):
    df = pd.DataFrame()
    for imgid in tqdm(imgids, total=len(imgids)):
        df_img = segment_image(dir_img, imgid, segmentator)
        df = df.append(df_img, ignore_index=True)

    return df

# Cell

def resize_image(img, sz):
    return cv2.resize(img, (sz, sz), interpolation=cv2.INTER_LINEAR)

# Cell

def crop_image(img, bbox, bmask=None):
    '''
    Args:
        img (np.array): Image to be cropped by ``bbox``.
        bbox (np.array): Bounding box in terms of [x0, y0, x1, y1].
        bmask (np.array, np.uint8): Binary mask for the cell.
    '''
    bbox = bbox.astype(np.int16)
    x0, y0, x1, y1 = bbox

    crop = img[y0:y1, x0:x1]

    if bmask is not None:
        crop = bmask[y0:y1, x0:x1][...,None] * crop

    return crop

# Cell

def remove_faint_greens(xs, crops, green_thres=64):
    assert len(xs) == len(crops)
    xs_out = []
    for x, crop in zip(xs, crops):
        if crop[...,1].max() > green_thres:
            xs_out.append(x)
    return xs_out

# Cell

def pad_to_square(img):
    '''
    Pad an image to a square size, centering it as much as possible.
    '''
    h, w, c = img.shape
    if h == w:
        return img
    elif h < w:
        img_padded = np.zeros((w, w, c), dtype=img.dtype)
        offset0 = (w - h) // 2
        offset1 = (w - h) - offset0
        img_padded[offset0:-offset1, :] = img.copy()
    else:
        img_padded = np.zeros((h, h, c), dtype=img.dtype)
        offset0 = (h - w) // 2
        offset1 = (h - w) - offset0
        img_padded[:, offset0:-offset1] = img.copy()
    return img_padded

# Cell
def load_seg_trn(pth_csv):
    '''
    Loads @dscettler8845's segmentation results for train set.
    '''
    df = pd.read_csv(pth_csv)

    df['cell_masks'] = df['cell_masks'].apply(ast.literal_eval)

    df['bboxes'] = (
        df['bboxes'].apply(lambda o: np.array(ast.literal_eval(o)))
    )

    return df

# Cell

def _split_cells(df_img):
    '''
    Expand a row representing a segmented image into multiple rows
    representing the cells in the image.
    '''
    imgid = df_img['ID'].item()
    sz = df_img['dimension'].item()
    rles = df_img['cell_masks'].item()
    bboxes = list(df_img['bboxes'].item())
    cellids = [f'{imgid}_{i}' for i in range(len(rles))]

    rles = [
        mutils.encode(rle_decode(rle, sz, sz)) for rle in rles]

    df = pd.DataFrame(
        {'Id': cellids, 'rle': rles, 'bbox': bboxes})
    df['Target'] = df_img['Label'].item()

    return df


def split_cells(df_seg):
    '''
    Args:
        df_seg (pd.DataFrame): Each row is an image.
        df_cells (pd.DataFrame): Each row is a cell.
    '''
    df_cells = (
        df_seg.groupby('ID')
        .apply(_split_cells).reset_index(drop=True)
    )
    return df_cells

# Cell

def generate_crops(df_cells, src, dst, out_sz=768):
    '''
    - Crop out each cell from its image.
    - Resize the crop to a square and save to disk.
    - Record the crop's maximum green channel value.
    '''
    df_cells = df_cells.copy(deep=True)

    max_greens = []

    imgids = df_cells['Id'].apply(lambda o: o.split('_')[0])

    for imgid, df_img in df_cells.groupby(imgids):
        img = load_RGBY_image(src, imgid)

        for cellid, df_cell in df_img.groupby('Id'):
            rle = df_cell['rle'].item()
            bbox = df_cell['bbox'].item()
            bmask = mutils.decode(rle)
            crop = crop_image(img, bbox, bmask=bmask)
            crop = pad_to_square(crop)

            max_green = np.max(crop[..., COLOR_INDEXS['green']])
            max_greens.append(max_green)

            if out_sz is not None:
                crop = resize_image(crop, out_sz)

            save_image(dst, cellid, crop)

    df_cells['max_green'] = max_greens

    return df_cells

# Cell

def fill_targets(row):
    row.Target = np.array(row.Target.split("|")).astype(np.int)
    for num in row.Target:
        name = LABEL_NAMES[int(num)]
        row.loc[name] = 1
    row.Target = "|".join(np.sort(np.unique(row.Target)).astype(str).tolist())
    return row


def generate_meta(dir_mdata, fname, dataset='train'):
    is_external = True if dataset == 'external' else False

    label_df = pd.read_feather(dir_mdata/'raw'/fname)
    for key in LABEL_NAMES.keys():
        label_df[LABEL_NAMES[key]] = 0
    meta_df = label_df.apply(fill_targets, axis=1)
    meta_df[EXTERNAL] = is_external

    if is_external:
        meta_df[ANTIBODY] = meta_df[ID].apply(lambda x: x.split('_')[0])

        clf = LabelEncoder()
        meta_df[ANTIBODY_CODE] = clf.fit_transform(meta_df[ANTIBODY])
        meta_df[ANTIBODY] = meta_df[ANTIBODY].astype(int)

    meta_dir = dir_mdata/'meta'
    meta_dir.mkdir(exist_ok=True, parents=True)
    meta_fname = meta_dir/f'{dataset}_meta.feather'
    meta_df.to_feather(meta_fname)

# Cell

from sklearn.model_selection import KFold,StratifiedKFold
# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/67819
from iterstrat.ml_stratifiers import MultilabelStratifiedKFold

# Cell

def get_meta():
    meta_dir = DATA_DIR/'meta'

    train_meta_fname = meta_dir/'train_meta.feather'
    train_meta = pd.read_feather(train_meta_fname)

    external_meta_fname = meta_dir/'external_meta.feather'
    if external_meta_fname.exists():
        external_meta = pd.read_feather(external_meta_fname)
    else:
        external_meta = None

    return train_meta, external_meta

# Cell

def create_split_file(data_set="train", name="train", num=None):
    split_dir = DATA_DIR/'split'
    split_dir.mkdir(exist_ok=True)

    if data_set=='train':
        ds = pd.read_feather(DATA_DIR/'meta'/'train_meta.feather')
    else:
        ds = pd.read_feather(DATA_DIR/'raw'/'test.feather')

    if num is None:
        split_df = ds
    elif name == "valid":
        split_df = ds.iloc[-num:].copy().reset_index()
    else:
        split_df = ds.iloc[:num]

    num = len(split_df)
    print("create split file: %s_%d" % (name, num))
    fname = split_dir/f"{name}_{num}.feather"
    split_df.to_feather(fname)


def create_random_split(dir_mdata, train_meta,
                        external_meta=None, n_splits=4, alias='random'):

    split_dir = dir_mdata/'split'/f'{alias}_folds{n_splits}'
    split_dir.mkdir(exist_ok=True, parents=True)

    kf = MultilabelStratifiedKFold(
        n_splits=n_splits, shuffle=True, random_state=100)

    train_indices_list, valid_indices_list = [], []
    for train_indices, valid_indices in kf.split(
        train_meta, train_meta[LABEL_NAME_LIST].values):

        train_indices_list.append(train_indices)
        valid_indices_list.append(valid_indices)

    ext_train_indices_list, ext_valid_indices_list = [], []
    if external_meta is not None:
        ext_kf = MultilabelStratifiedKFold(
            n_splits=n_splits, shuffle=True, random_state=100)

        for ext_train_indices, ext_valid_indices in ext_kf.split(
            external_meta, external_meta[LABEL_NAME_LIST].values):

            ext_train_indices_list.append(ext_train_indices)
            ext_valid_indices_list.append(ext_valid_indices)

    for idx in range(n_splits):
        train_split_df = train_meta.loc[train_indices_list[idx]]
        valid_split_df = train_meta.loc[valid_indices_list[idx]]

        if external_meta is not None:
            train_split_df = pd.concat(
                (train_split_df,
                 external_meta.loc[ext_train_indices_list[idx]]),
                ignore_index=True)

            valid_split_df = pd.concat(
                (valid_split_df,
                 external_meta.loc[ext_valid_indices_list[idx]]),
                ignore_index=True)

            train_split_df = train_split_df[[ID, TARGET, EXTERNAL, ANTIBODY, ANTIBODY_CODE] + LABEL_NAME_LIST]
            valid_split_df = valid_split_df[[ID, TARGET, EXTERNAL, ANTIBODY, ANTIBODY_CODE] + LABEL_NAME_LIST]

        if idx == 0:
            for name in LABEL_NAMES.values():
                print(name, (train_split_df[name]==1).sum(), (valid_split_df[name]==1).sum())

        fname = split_dir/f'random_train_cv{idx}.feather'
        print("create split file: %s, shape: %s" % (fname, str(train_split_df.shape)))
        train_split_df.reset_index().to_feather(fname)

        fname = split_dir/f'random_valid_cv{idx}.feather'
        print("create split file: %s, shape: %s" % (fname, str(valid_split_df.shape)))
        valid_split_df.reset_index().to_feather(fname)


def load_match_info():
    train_meta_df = pd.read_csv(opj(DATA_DIR, 'meta', 'train_meta.csv'))
    external_meta_df = pd.read_csv(opj(DATA_DIR, 'meta', 'external_meta.csv'))
    meta_df = pd.concat((train_meta_df[[ID, TARGET]], external_meta_df[[ID, TARGET]]), axis=0, ignore_index=True)

    match_df = pd.read_csv(opj(DATA_DIR, 'meta', 'train_match_external.csv.gz'), usecols=['Train', 'Extra'])
    match_df = pd.merge(match_df, meta_df.rename(columns={ID: 'Train'}), on='Train', how='left')
    match_df = match_df.rename(columns={TARGET: 'Train_%s' % TARGET})
    match_df = pd.merge(match_df, meta_df.rename(columns={ID: 'Extra'}), on='Extra', how='left')
    match_df = match_df.rename(columns={TARGET: 'Extra_%s' % TARGET})

    match_df['Equal'] = match_df['Train_%s' % TARGET] == match_df['Extra_%s' % TARGET]
    match_df = match_df[['Train', 'Extra', 'Train_%s' % TARGET, 'Extra_%s' % TARGET, 'Equal']]
    return match_df


def generate_noleak_split(n_splits=5):
    match_df = load_match_info()
    match_img_ids = match_df[~match_df['Equal']]['Extra'].unique()

    target_dir = opj(DATA_DIR, 'split', 'random_ext_noleak_clean_folds5')
    os.makedirs(target_dir, exist_ok=True)
    for idx in range(n_splits):
        train_split_df = pd.read_csv(opj(DATA_DIR, 'split', 'random_ext_folds5', 'random_train_cv%d.csv' % idx))
        start_num = len(train_split_df)
        train_split_df = train_split_df[~train_split_df[ID].isin(match_img_ids)]
        end_num = len(train_split_df)
        print('trainset remove num: %d' % (start_num - end_num))

        valid_split_df = pd.read_csv(opj(DATA_DIR, 'split', 'random_ext_folds5', 'random_valid_cv%d.csv' % idx))
        start_num = len(valid_split_df)
        leak_img_ids = match_df[(match_df['Equal']) & (match_df['Train'].isin(train_split_df[ID].values))]['Extra'].unique()
        valid_split_df = valid_split_df[(~valid_split_df[ID].isin(match_img_ids)) & (~valid_split_df[ID].isin(leak_img_ids))]
        end_num = len(valid_split_df)
        print('validset remove num: %d' % (start_num - end_num))

        fname = opj(target_dir, 'random_train_cv%d.csv' % idx)
        print(fname, train_split_df.shape)
        train_split_df.to_csv(fname, index=False)

        fname = opj(target_dir, 'random_valid_cv%d.csv' % idx)
        print(fname, valid_split_df.shape)
        valid_split_df.to_csv(fname, index=False)

# Cell

def get_img_mean_std(img_dir, color, img_mean, img_std):
    img_list = os.listdir(img_dir)
    img_list = [i for i in img_list if i.count(color) > 0]
#     print(img_dir, len(img_list))
    for img_id in tqdm(img_list):
        image_path = opj(img_dir, img_id)
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) / 255

        m = img.mean()
        s = img.std()
        img_mean.append(m)
        img_std.append(s)
    return img_mean, img_std