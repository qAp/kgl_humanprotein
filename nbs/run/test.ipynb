{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp run.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../../HPA-competition-solutions/bestfitting/src/run/test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from config.config import *\n",
    "from utils.common_util import *\n",
    "from networks.imageclsnet import init_network\n",
    "from datasets.protein_dataset import ProteinDataset\n",
    "from utils.augment_util import *\n",
    "from utils.log_util import Logger\n",
    "\n",
    "datasets_names = ['test', 'val']\n",
    "split_names = ['random_ext_folds5', 'random_ext_noleak_clean_folds5']\n",
    "augment_list = ['default', 'flipud', 'fliplr','transpose', 'flipud_lr',\n",
    "                'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose']\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Protein Classification')\n",
    "parser.add_argument('--out_dir', type=str, help='destination where predicted result should be saved')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for predicting (default: 0)')\n",
    "parser.add_argument('--arch', default='class_densenet121_dropout', type=str,\n",
    "                    help='model architecture (default: class_densenet121_dropout)')\n",
    "parser.add_argument('--num_classes', default=28, type=int, help='number of classes (default: 28)')\n",
    "parser.add_argument('--in_channels', default=4, type=int, help='in channels (default: 4)')\n",
    "parser.add_argument('--img_size', default=768, type=int, help='image size (default: 768)')\n",
    "parser.add_argument('--crop_size', default=512, type=int, help='crop size (default: 512)')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='train mini-batch size (default: 32)')\n",
    "parser.add_argument('--workers', default=3, type=int, help='number of data loading workers (default: 3)')\n",
    "parser.add_argument('--fold', default=0, type=int, help='index of fold (default: 0)')\n",
    "parser.add_argument('--augment', default='default', type=str, help='test augmentation (default: default)')\n",
    "parser.add_argument('--seed', default=100, type=int, help='random seed (default: 100)')\n",
    "parser.add_argument('--seeds', default=None, type=str, help='predict seed')\n",
    "parser.add_argument('--dataset', default='test', type=str, choices=datasets_names,\n",
    "                    help='dataset options: ' + ' | '.join(datasets_names) + ' (default: test)')\n",
    "parser.add_argument('--split_name', default='random_ext_folds5', type=str, choices=split_names,\n",
    "                    help='split name options: ' + ' | '.join(split_names) + ' (default: random_ext_folds5)')\n",
    "parser.add_argument('--predict_epoch', default=None, type=int, help='number epoch to predict')\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    log_out_dir = opj(RESULT_DIR, 'logs', args.out_dir, 'fold%d' % args.fold)\n",
    "    if not ope(log_out_dir):\n",
    "        os.makedirs(log_out_dir)\n",
    "    log = Logger()\n",
    "    log.open(opj(log_out_dir, 'log.submit.txt'), mode='a')\n",
    "\n",
    "    args.predict_epoch = 'final' if args.predict_epoch is None else '%03d' % args.predict_epoch\n",
    "    network_path = opj(RESULT_DIR, 'models', args.out_dir, 'fold%d' % args.fold, '%s.pth' % args.predict_epoch)\n",
    "\n",
    "    submit_out_dir = opj(RESULT_DIR, 'submissions', args.out_dir, 'fold%d' % args.fold, 'epoch_%s' % args.predict_epoch)\n",
    "    log.write(\">> Creating directory if it does not exist:\\n>> '{}'\\n\".format(submit_out_dir))\n",
    "    if not ope(submit_out_dir):\n",
    "        os.makedirs(submit_out_dir)\n",
    "\n",
    "    # setting up the visible GPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "    args.augment = args.augment.split(',')\n",
    "    for augment in args.augment:\n",
    "        if augment not in augment_list:\n",
    "            raise ValueError('Unsupported or unknown test augmentation: {}!'.format(augment))\n",
    "\n",
    "    model_params = {}\n",
    "    model_params['architecture'] = args.arch\n",
    "    model_params['num_classes'] = args.num_classes\n",
    "    model_params['in_channels'] = args.in_channels\n",
    "    model = init_network(model_params)\n",
    "\n",
    "    log.write(\">> Loading network:\\n>>>> '{}'\\n\".format(network_path))\n",
    "    checkpoint = torch.load(network_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    log.write(\">>>> loaded network:\\n>>>> epoch {}\\n\".format(checkpoint['epoch']))\n",
    "\n",
    "    # moving network to gpu and eval mode\n",
    "    model = DataParallel(model)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    # Data loading code\n",
    "    dataset = args.dataset\n",
    "    if dataset == 'test':\n",
    "        test_split_file = opj(DATA_DIR, 'split', 'test_11702.csv')\n",
    "    elif dataset == 'val':\n",
    "        test_split_file = opj(DATA_DIR, 'split', args.split_name, 'random_valid_cv%d.csv' % args.fold)\n",
    "    else:\n",
    "        raise ValueError('Unsupported or unknown dataset: {}!'.format(dataset))\n",
    "    test_dataset = ProteinDataset(\n",
    "        test_split_file,\n",
    "        img_size=args.img_size,\n",
    "        is_trainset=(dataset != 'test'),\n",
    "        return_label=False,\n",
    "        in_channels=args.in_channels,\n",
    "        transform=None,\n",
    "        crop_size=args.crop_size,\n",
    "        random_crop=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    seeds = [args.seed] if args.seeds is None else [int(i) for i in args.seeds.split(',')]\n",
    "    for seed in seeds:\n",
    "        test_dataset.random_crop = (seed != 0)\n",
    "        for augment in args.augment:\n",
    "            test_loader.dataset.transform = eval('augment_%s' % augment)\n",
    "            if args.crop_size > 0:\n",
    "                sub_submit_out_dir = opj(submit_out_dir, '%s_seed%d' % (augment, seed))\n",
    "            else:\n",
    "                sub_submit_out_dir = opj(submit_out_dir, augment)\n",
    "            if not ope(sub_submit_out_dir):\n",
    "                os.makedirs(sub_submit_out_dir)\n",
    "            with torch.no_grad():\n",
    "                predict(test_loader, model, sub_submit_out_dir, dataset)\n",
    "\n",
    "def predict(test_loader, model, submit_out_dir, dataset):\n",
    "    all_probs = []\n",
    "    img_ids = np.array(test_loader.dataset.img_ids)\n",
    "    for it, iter_data in tqdm(enumerate(test_loader, 0), total=len(test_loader)):\n",
    "        images, indices = iter_data\n",
    "        images = Variable(images.cuda(), volatile=True)\n",
    "        outputs = model(images)\n",
    "        logits = outputs\n",
    "\n",
    "        probs = F.sigmoid(logits).data\n",
    "        all_probs += probs.cpu().numpy().tolist()\n",
    "    img_ids = img_ids[:len(all_probs)]\n",
    "    all_probs = np.array(all_probs).reshape(len(img_ids), -1)\n",
    "\n",
    "    np.save(opj(submit_out_dir, 'prob_%s.npy' % dataset), all_probs)\n",
    "\n",
    "    result_df = prob_to_result(all_probs, img_ids)\n",
    "    result_df.to_csv(opj(submit_out_dir, 'results_%s.csv.gz' % dataset), index=False, compression='gzip')\n",
    "\n",
    "def prob_to_result(probs, img_ids, th=0.5):\n",
    "    probs = probs.copy()\n",
    "    probs[np.arange(len(probs)), np.argmax(probs, axis=1)] = 1\n",
    "\n",
    "    pred_list = []\n",
    "    for line in probs:\n",
    "        s = ' '.join(list([str(i) for i in np.nonzero(line > th)[0]]))\n",
    "        pred_list.append(s)\n",
    "    result_df = pd.DataFrame({ID: img_ids, PREDICTED: pred_list})\n",
    "    return result_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('%s: calling main function ... \\n' % os.path.basename(__file__))\n",
    "    main()\n",
    "    print('\\nsuccess!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
