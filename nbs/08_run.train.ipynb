{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `run.train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp run.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on collie.local\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "from torch.backends import cudnn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgl_humanprotein.config.config import *\n",
    "from kgl_humanprotein.utils.common_util import *\n",
    "from kgl_humanprotein.networks.imageclsnet import init_network\n",
    "from kgl_humanprotein.datasets.protein_dataset import ProteinDataset\n",
    "from kgl_humanprotein.utils.augment_util import train_multi_augment2\n",
    "from kgl_humanprotein.layers.loss import *\n",
    "from kgl_humanprotein.layers.scheduler import *\n",
    "from kgl_humanprotein.utils.log_util import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "loss_names = ['FocalSymmetricLovaszHardLogLoss']\n",
    "split_names = ['random_ext_folds5', 'random_ext_noleak_clean_folds5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Protein Classification')\n",
    "parser.add_argument('--out_dir', type=str, help='destination where trained network should be saved')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for training (default: 0)')\n",
    "parser.add_argument('--arch', default='class_densenet121_dropout', type=str,\n",
    "                    help='model architecture (default: class_densenet121_dropout)')\n",
    "parser.add_argument('--num_classes', default=28, type=int, help='number of classes (default: 28)')\n",
    "parser.add_argument('--in_channels', default=4, type=int, help='in channels (default: 4)')\n",
    "parser.add_argument('--loss', default='FocalSymmetricLovaszHardLogLoss', choices=loss_names, type=str,\n",
    "                    help='loss function: ' + ' | '.join(loss_names) + ' (deafault: FocalSymmetricLovaszHardLogLoss)')\n",
    "parser.add_argument('--scheduler', default='Adam45', type=str, help='scheduler name')\n",
    "parser.add_argument('--epochs', default=55, type=int, help='number of total epochs to run (default: 55)')\n",
    "parser.add_argument('--img_size', default=768, type=int, help='image size (default: 768)')\n",
    "parser.add_argument('--crop_size', default=512, type=int, help='crop size (default: 512)')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='train mini-batch size (default: 32)')\n",
    "parser.add_argument('--workers', default=3, type=int, help='number of data loading workers (default: 3)')\n",
    "parser.add_argument('--split_name', default='random_ext_folds5', type=str, choices=split_names,\n",
    "                    help='split name options: ' + ' | '.join(split_names) + ' (default: random_ext_folds5)')\n",
    "parser.add_argument('--fold', default=0, type=int, help='index of fold (default: 0)')\n",
    "parser.add_argument('--clipnorm', default=1, type=int, help='clip grad norm')\n",
    "parser.add_argument('--resume', default=None, type=str, help='name of the latest checkpoint (default: None)')\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    log_out_dir = opj(RESULT_DIR, 'logs', args.out_dir, 'fold%d' % args.fold)\n",
    "    if not ope(log_out_dir):\n",
    "        os.makedirs(log_out_dir)\n",
    "    log = Logger()\n",
    "    log.open(opj(log_out_dir, 'log.train.txt'), mode='a')\n",
    "\n",
    "    model_out_dir = opj(RESULT_DIR, 'models', args.out_dir, 'fold%d' % args.fold)\n",
    "    log.write(\">> Creating directory if it does not exist:\\n>> '{}'\\n\".format(model_out_dir))\n",
    "    if not ope(model_out_dir):\n",
    "        os.makedirs(model_out_dir)\n",
    "\n",
    "    # set cuda visible device\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # set random seeds\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    model_params = {}\n",
    "    model_params['architecture'] = args.arch\n",
    "    model_params['num_classes'] = args.num_classes\n",
    "    model_params['in_channels'] = args.in_channels\n",
    "    model = init_network(model_params)\n",
    "\n",
    "    # move network to gpu\n",
    "    model = DataParallel(model)\n",
    "    model.cuda()\n",
    "\n",
    "    # define loss function (criterion)\n",
    "    try:\n",
    "        criterion = eval(args.loss)().cuda()\n",
    "    except:\n",
    "        raise(RuntimeError(\"Loss {} not available!\".format(args.loss)))\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_loss = 1e5\n",
    "    best_epoch = 0\n",
    "    best_focal = 1e5\n",
    "\n",
    "    # define scheduler\n",
    "    try:\n",
    "        scheduler = eval(args.scheduler)()\n",
    "    except:\n",
    "        raise (RuntimeError(\"Scheduler {} not available!\".format(args.scheduler)))\n",
    "    optimizer = scheduler.schedule(model, start_epoch, args.epochs)[0]\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        args.resume = os.path.join(model_out_dir, args.resume)\n",
    "        if os.path.isfile(args.resume):\n",
    "            # load checkpoint weights and update model and optimizer\n",
    "            log.write(\">> Loading checkpoint:\\n>> '{}'\\n\".format(args.resume))\n",
    "\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_epoch = checkpoint['best_epoch']\n",
    "            best_focal = checkpoint['best_score']\n",
    "            model.module.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "            optimizer_fpath = args.resume.replace('.pth', '_optim.pth')\n",
    "            if ope(optimizer_fpath):\n",
    "                log.write(\">> Loading checkpoint:\\n>> '{}'\\n\".format(optimizer_fpath))\n",
    "                optimizer.load_state_dict(torch.load(optimizer_fpath)['optimizer'])\n",
    "            log.write(\">>>> loaded checkpoint:\\n>>>> '{}' (epoch {})\\n\".format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            log.write(\">> No checkpoint found at '{}'\\n\".format(args.resume))\n",
    "\n",
    "    # Data loading code\n",
    "    train_transform = train_multi_augment2\n",
    "    train_split_file = opj(DATA_DIR, 'split', args.split_name, 'random_train_cv%d.csv' % args.fold)\n",
    "    train_dataset = ProteinDataset(\n",
    "        train_split_file,\n",
    "        img_size=args.img_size,\n",
    "        is_trainset=True,\n",
    "        return_label=True,\n",
    "        in_channels=args.in_channels,\n",
    "        transform=train_transform,\n",
    "        crop_size=args.crop_size,\n",
    "        random_crop=True,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    valid_split_file = opj(DATA_DIR, 'split', args.split_name, 'random_valid_cv%d.csv' % args.fold)\n",
    "    valid_dataset = ProteinDataset(\n",
    "        valid_split_file,\n",
    "        img_size=args.img_size,\n",
    "        is_trainset=True,\n",
    "        return_label=True,\n",
    "        in_channels=args.in_channels,\n",
    "        transform=None,\n",
    "        crop_size=args.crop_size,\n",
    "        random_crop=False,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    focal_loss = FocalLoss().cuda()\n",
    "    log.write('** start training here! **\\n')\n",
    "    log.write('\\n')\n",
    "    log.write('epoch    iter      rate     |  train_loss/acc  |    valid_loss/acc/focal/kaggle     |best_epoch/best_focal|  min \\n')\n",
    "    log.write('-----------------------------------------------------------------------------------------------------------------\\n')\n",
    "    start_epoch += 1\n",
    "    for epoch in range(start_epoch, args.epochs + 1):\n",
    "        end = time.time()\n",
    "\n",
    "        # set manual seeds per epoch\n",
    "        np.random.seed(epoch)\n",
    "        torch.manual_seed(epoch)\n",
    "        torch.cuda.manual_seed_all(epoch)\n",
    "\n",
    "        # adjust learning rate for each epoch\n",
    "        lr_list = scheduler.step(model, epoch, args.epochs)\n",
    "        lr = lr_list[0]\n",
    "\n",
    "        # train for one epoch on train set\n",
    "        iter, train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, clipnorm=args.clipnorm, lr=lr)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_acc, valid_focal_loss, kaggle_score = validate(valid_loader, model, criterion, epoch, focal_loss)\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = valid_focal_loss < best_focal\n",
    "        best_loss = min(valid_focal_loss, best_loss)\n",
    "        best_epoch = epoch if is_best else best_epoch\n",
    "        best_focal = valid_focal_loss if is_best else best_focal\n",
    "\n",
    "        print('\\r', end='', flush=True)\n",
    "        log.write('%5.1f   %5d    %0.6f   |  %0.4f  %0.4f  |    %0.4f  %6.4f %6.4f %6.4f    |  %6.1f    %6.4f   | %3.1f min \\n' % \\\n",
    "                  (epoch, iter + 1, lr, train_loss, train_acc, valid_loss, valid_acc, valid_focal_loss, kaggle_score,\n",
    "                   best_epoch, best_focal, (time.time() - end) / 60))\n",
    "\n",
    "        save_model(model, is_best, model_out_dir, optimizer=optimizer, epoch=epoch, best_epoch=best_epoch, best_focal=best_focal)\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, clipnorm=1, lr=1e-5):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    num_its = len(train_loader)\n",
    "    end = time.time()\n",
    "    iter = 0\n",
    "    print_freq = 1\n",
    "    for iter, iter_data in enumerate(train_loader, 0):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # zero out gradients so we can accumulate new ones over batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, labels, indices = iter_data\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels, epoch=epoch)\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clipnorm)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        logits = outputs\n",
    "        probs = F.sigmoid(logits)\n",
    "        acc = multi_class_acc(probs, labels)\n",
    "        accuracy.update(acc.item())\n",
    "\n",
    "        if (iter + 1) % print_freq == 0 or iter == 0 or (iter + 1) == num_its:\n",
    "            print('\\r%5.1f   %5d    %0.6f   |  %0.4f  %0.4f  | ... ' % \\\n",
    "                  (epoch - 1 + (iter + 1) / num_its, iter + 1, lr, losses.avg, accuracy.avg), \\\n",
    "                  end='', flush=True)\n",
    "\n",
    "    return iter, losses.avg, accuracy.avg\n",
    "\n",
    "def validate(valid_loader, model, criterion, epoch, focal_loss, threshold=0.5):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    probs_list = []\n",
    "    labels_list = []\n",
    "    logits_list = []\n",
    "\n",
    "    end = time.time()\n",
    "    for it, iter_data in enumerate(valid_loader, 0):\n",
    "        images, labels, indices = iter_data\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels, epoch=epoch)\n",
    "\n",
    "        logits = outputs\n",
    "        probs = F.sigmoid(logits)\n",
    "        acc = multi_class_acc(probs, labels)\n",
    "\n",
    "        probs_list.append(probs.cpu().detach().numpy())\n",
    "        labels_list.append(labels.cpu().detach().numpy())\n",
    "        logits_list.append(logits.cpu().detach().numpy())\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        accuracy.update(acc.item())\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    probs = np.vstack(probs_list)\n",
    "    y_true = np.vstack(labels_list)\n",
    "    logits = np.vstack(logits_list)\n",
    "    valid_focal_loss = focal_loss.forward(torch.from_numpy(logits), torch.from_numpy(y_true))\n",
    "\n",
    "    y_pred = probs > threshold\n",
    "    kaggle_score = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return losses.avg, accuracy.avg, valid_focal_loss, kaggle_score\n",
    "\n",
    "def save_model(model, is_best, model_out_dir, optimizer=None, epoch=None, best_epoch=None, best_focal=None):\n",
    "    if type(model) == DataParallel:\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    for key in state_dict.keys():\n",
    "        state_dict[key] = state_dict[key].cpu()\n",
    "\n",
    "    model_fpath = opj(model_out_dir, '%03d.pth' % epoch)\n",
    "    torch.save({\n",
    "        'save_dir': model_out_dir,\n",
    "        'state_dict': state_dict,\n",
    "        'best_epoch': best_epoch,\n",
    "        'epoch': epoch,\n",
    "        'best_score': best_focal,\n",
    "    }, model_fpath)\n",
    "\n",
    "    optim_fpath = opj(model_out_dir, '%03d_optim.pth' % epoch)\n",
    "    if optimizer is not None:\n",
    "        torch.save({\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, optim_fpath)\n",
    "\n",
    "    if is_best:\n",
    "        best_model_fpath = opj(model_out_dir, 'final.pth')\n",
    "        shutil.copyfile(model_fpath, best_model_fpath)\n",
    "        if optimizer is not None:\n",
    "            best_optim_fpath = opj(model_out_dir, 'final_optim.pth')\n",
    "            shutil.copyfile(optim_fpath, best_optim_fpath)\n",
    "\n",
    "def multi_class_acc(preds, targs, th=0.5):\n",
    "    preds = (preds > th).int()\n",
    "    targs = targs.int()\n",
    "    return (preds == targs).float().mean()\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('%s: calling main function ... \\n' % os.path.basename(__file__))\n",
    "    main()\n",
    "    print('\\nsuccess!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
