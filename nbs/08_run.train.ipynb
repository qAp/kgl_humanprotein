{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `run.train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp run.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "from torch.backends import cudnn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from kgl_humanprotein.config.config import *\n",
    "from kgl_humanprotein.utils.common_util import *\n",
    "from kgl_humanprotein.data_process import generate_meta, create_random_split\n",
    "from kgl_humanprotein.networks.imageclsnet import init_network\n",
    "from kgl_humanprotein.datasets.protein_dataset import ProteinDataset\n",
    "from kgl_humanprotein.utils.augment_util import train_multi_augment2\n",
    "from kgl_humanprotein.layers.loss import *\n",
    "from kgl_humanprotein.layers.scheduler import *\n",
    "from kgl_humanprotein.utils.log_util import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the subsets' meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def combine_subsets_metadata(dir_data, n_subsets=5):\n",
    "    df_cells = pd.DataFrame()\n",
    "    for isubset in range(n_subsets):\n",
    "        print(f'\\rProcessing subset {isubset}...', end='', flush=True)\n",
    "        pth_feather = (dir_data / \n",
    "                       f'humanpro-train-cells-subset{isubset}' /\n",
    "                       f'humanpro_train_cells_subset{isubset}' / 'train/train.feather')\n",
    "        \n",
    "        df = pd.read_feather(pth_feather)\n",
    "        df['subset'] = isubset\n",
    "        \n",
    "        df_cells = df_cells.append(df, ignore_index=True)\n",
    "    return df_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = Path('../../kgl_humanprotein_data')\n",
    "dir_mdata = Path('mdata')\n",
    "n_subsets = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subset 4...CPU times: user 25.2 ms, sys: 20.1 ms, total: 45.4 ms\n",
      "Wall time: 88.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_cells = combine_subsets_metadata(dir_data, n_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_mdata_raw = dir_mdata/'raw'\n",
    "dir_mdata_raw.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df_cells.to_feather(dir_mdata_raw/'train.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *bestfitting*'s code, this generates the \"meta\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 242 ms, sys: 12.8 ms, total: 255 ms\n",
      "Wall time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate_meta(dir_mdata, 'train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nucleoplasm 343 85\n",
      "Nuclear membrane 59 14\n",
      "Nucleoli 60 15\n",
      "Nucleoli fibrillar center 31 8\n",
      "Nuclear speckles 11 2\n",
      "Nuclear bodies 49 12\n",
      "Endoplasmic reticulum 64 16\n",
      "Golgi apparatus 82 20\n",
      "Intermediate filaments 0 0\n",
      "Actin filaments 38 10\n",
      "Microtubules 56 14\n",
      "Mitotic spindle 0 0\n",
      "Centrosome 33 8\n",
      "Plasma membrane 137 34\n",
      "Mitochondria 12 3\n",
      "Aggresome 20 5\n",
      "Cytosol 96 24\n",
      "Vesicles and punctate cytosolic patterns 0 0\n",
      "Negative 0 0\n",
      "create split file: mdata/split/random_folds5/random_train_cv0.feather, shape: (606, 26)\n",
      "create split file: mdata/split/random_folds5/random_valid_cv0.feather, shape: (159, 26)\n",
      "create split file: mdata/split/random_folds5/random_train_cv1.feather, shape: (612, 26)\n",
      "create split file: mdata/split/random_folds5/random_valid_cv1.feather, shape: (153, 26)\n",
      "create split file: mdata/split/random_folds5/random_train_cv2.feather, shape: (617, 26)\n",
      "create split file: mdata/split/random_folds5/random_valid_cv2.feather, shape: (148, 26)\n",
      "create split file: mdata/split/random_folds5/random_train_cv3.feather, shape: (612, 26)\n",
      "create split file: mdata/split/random_folds5/random_valid_cv3.feather, shape: (153, 26)\n",
      "create split file: mdata/split/random_folds5/random_train_cv4.feather, shape: (613, 26)\n",
      "create split file: mdata/split/random_folds5/random_valid_cv4.feather, shape: (152, 26)\n",
      "CPU times: user 98.9 ms, sys: 24.6 ms, total: 124 ms\n",
      "Wall time: 127 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_meta = pd.read_feather(dir_mdata/'meta'/'train_meta.feather')\n",
    "create_random_split(dir_mdata, train_meta, n_splits=5, alias='random')\n",
    "del train_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "loss_names = ['FocalSymmetricLovaszHardLogLoss']\n",
    "split_names = ['random_ext_folds5', \n",
    "               'random_ext_noleak_clean_folds5']\n",
    "\n",
    "\n",
    "def main_training(dir_data, dir_mdata, dir_results, out_dir, gpu_id='0', arch='class_densenet121_dropout',\n",
    "         num_classes=19, in_channels=4, \n",
    "         loss='FocalSymmetricLovaszHardLogLoss',\n",
    "         scheduler='Adam45', epochs=55, img_size=768, \n",
    "         crop_size=512, batch_size=32, workers=3, pin_memory=True,\n",
    "         split_name='random_ext_folds5', fold=0, clipnorm=1, \n",
    "         resume=None):\n",
    "    '''\n",
    "    PyTorch Protein Classification.  Main training function.\n",
    "    \n",
    "    Args:\n",
    "        out_dir (str): \n",
    "            Destination where trained network should be saved.\n",
    "        gpu_id (str): GPU id used for training. Default: '0'\n",
    "        arch (str): Model architecture.  \n",
    "            Default: ``'class_densenet121_dropout'``\n",
    "        num_classes (int): Number of classes. Default: 19 \n",
    "        in_channels (int): In channels. Default: 4\n",
    "        loss (str, optional): Loss function. \n",
    "            One of ``'FocalSymmetricLovaszHardLogLoss'``. \n",
    "            Default: ``'FocalSymmetricLovaszHardLogLoss'``\n",
    "        scheduler (str): Scheduler name. Default: ``'Adam45'``\n",
    "        epochs (int): Number of total epochs to run. Default: 55\n",
    "        img_size (int): Image size.  Default: 768\n",
    "        crop_size (int): Crop size.  Default: 512\n",
    "        batch_size (int): Train mini-batch size. Default: 32\n",
    "        workers (int): Number of data loading workers. Default: 3\n",
    "        pin_memory (bool): DataLoader's ``pin_memory`` argument.\n",
    "        split_name (str, optional): Split name.  \n",
    "            One of: ``'random_ext_folds5'``, \n",
    "            or ``'random_ext_noleak_clean_folds5'``. \n",
    "            Default: ``'random_ext_folds5'``\n",
    "        fold (int): Index of fold. Default: 0\n",
    "        clipnorm (int): Clip grad norm'. Default: 1\n",
    "        resume (str): Name of the latest checkpoint. Default: None\n",
    "    '''\n",
    "    log_out_dir = opj(dir_results, 'logs', out_dir, 'fold%d' % fold)\n",
    "    if not ope(log_out_dir):\n",
    "        os.makedirs(log_out_dir)\n",
    "    log = Logger()\n",
    "    log.open(opj(log_out_dir, 'log.train.txt'), mode='a')\n",
    "\n",
    "    model_out_dir = opj(dir_results, 'models', out_dir, 'fold%d' % fold)\n",
    "    log.write(\">> Creating directory if it does not exist:\\n>> '{}'\\n\".format(model_out_dir))\n",
    "    if not ope(model_out_dir):\n",
    "        os.makedirs(model_out_dir)\n",
    "\n",
    "    # set cuda visible device\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # set random seeds\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    model_params = {}\n",
    "    model_params['architecture'] = arch\n",
    "    model_params['num_classes'] = num_classes\n",
    "    model_params['in_channels'] = in_channels\n",
    "    model = init_network(model_params)\n",
    "\n",
    "    # move network to gpu\n",
    "    model = DataParallel(model)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # define loss function (criterion)\n",
    "    try:\n",
    "        criterion = eval(loss)().to(DEVICE)\n",
    "    except:\n",
    "        raise(RuntimeError(\"Loss {} not available!\".format(loss)))\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_loss = 1e5\n",
    "    best_epoch = 0\n",
    "    best_focal = 1e5\n",
    "\n",
    "    # define scheduler\n",
    "    try:\n",
    "        scheduler = eval(scheduler)()\n",
    "    except:\n",
    "        raise (RuntimeError(\"Scheduler {} not available!\".format(scheduler)))\n",
    "    optimizer = scheduler.schedule(model, start_epoch, epochs)[0]\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if resume:\n",
    "        resume = os.path.join(model_out_dir, resume)\n",
    "        if os.path.isfile(resume):\n",
    "            # load checkpoint weights and update model and optimizer\n",
    "            log.write(\">> Loading checkpoint:\\n>> '{}'\\n\".format(resume))\n",
    "\n",
    "            checkpoint = torch.load(resume)\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_epoch = checkpoint['best_epoch']\n",
    "            best_focal = checkpoint['best_score']\n",
    "            model.module.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "            optimizer_fpath = resume.replace('.pth', '_optim.pth')\n",
    "            if ope(optimizer_fpath):\n",
    "                log.write(\">> Loading checkpoint:\\n>> '{}'\\n\".format(optimizer_fpath))\n",
    "                optimizer.load_state_dict(torch.load(optimizer_fpath)['optimizer'])\n",
    "            log.write(\">>>> loaded checkpoint:\\n>>>> '{}' (epoch {})\\n\".format(resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            log.write(\">> No checkpoint found at '{}'\\n\".format(resume))\n",
    "\n",
    "    # Data loading code\n",
    "    train_transform = train_multi_augment2\n",
    "    train_split_file = (dir_mdata/'split'/split_name/\n",
    "                        f'random_train_cv{fold}.feather')\n",
    "    train_dataset = ProteinDataset(\n",
    "        dir_data,\n",
    "        train_split_file,\n",
    "        img_size=img_size,\n",
    "        is_trainset=True,\n",
    "        return_label=True,\n",
    "        in_channels=in_channels,\n",
    "        transform=train_transform,\n",
    "        crop_size=crop_size,\n",
    "        random_crop=True)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory)\n",
    "    \n",
    "    valid_split_file = (dir_mdata/'split'/split_name/\n",
    "                        f'random_valid_cv{fold}.feather')\n",
    "    \n",
    "    valid_dataset = ProteinDataset(\n",
    "        dir_data,\n",
    "        valid_split_file,\n",
    "        img_size=img_size,\n",
    "        is_trainset=True,\n",
    "        return_label=True,\n",
    "        in_channels=in_channels,\n",
    "        transform=None,\n",
    "        crop_size=crop_size,\n",
    "        random_crop=False)\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        batch_size=batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory)\n",
    "\n",
    "    focal_loss = FocalLoss().to(DEVICE)\n",
    "    log.write('** start training here! **\\n')\n",
    "    log.write('\\n')\n",
    "    log.write('epoch    iter      rate     |  train_loss/acc  |    valid_loss/acc/focal/kaggle     |best_epoch/best_focal|  min \\n')\n",
    "    log.write('-----------------------------------------------------------------------------------------------------------------\\n')\n",
    "    start_epoch += 1\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        end = time.time()\n",
    "\n",
    "        # set manual seeds per epoch\n",
    "        np.random.seed(epoch)\n",
    "        torch.manual_seed(epoch)\n",
    "        torch.cuda.manual_seed_all(epoch)\n",
    "\n",
    "        # adjust learning rate for each epoch\n",
    "        lr_list = scheduler.step(model, epoch, epochs)\n",
    "        lr = lr_list[0]\n",
    "\n",
    "        # train for one epoch on train set\n",
    "        iter, train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, clipnorm=clipnorm, lr=lr)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valid_loss, valid_acc, valid_focal_loss, kaggle_score = validate(valid_loader, model, criterion, epoch, focal_loss)\n",
    "\n",
    "        # remember best loss and save checkpoint\n",
    "        is_best = valid_focal_loss < best_focal\n",
    "        best_loss = min(valid_focal_loss, best_loss)\n",
    "        best_epoch = epoch if is_best else best_epoch\n",
    "        best_focal = valid_focal_loss if is_best else best_focal\n",
    "\n",
    "        print('\\r', end='', flush=True)\n",
    "        log.write('%5.1f   %5d    %0.6f   |  %0.4f  %0.4f  |    %0.4f  %6.4f %6.4f %6.4f    |  %6.1f    %6.4f   | %3.1f min \\n' % \\\n",
    "                  (epoch, iter + 1, lr, train_loss, train_acc, valid_loss, valid_acc, valid_focal_loss, kaggle_score,\n",
    "                   best_epoch, best_focal, (time.time() - end) / 60))\n",
    "\n",
    "        save_model(model, is_best, model_out_dir, optimizer=optimizer, epoch=epoch, best_epoch=best_epoch, best_focal=best_focal)\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, clipnorm=1, lr=1e-5):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    num_its = len(train_loader)\n",
    "    end = time.time()\n",
    "    iter = 0\n",
    "    print_freq = 1\n",
    "    for iter, iter_data in enumerate(train_loader, 0):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # zero out gradients so we can accumulate new ones over batches\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, labels, indices = iter_data\n",
    "        images = Variable(images.to(DEVICE))\n",
    "        labels = Variable(labels.to(DEVICE))\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels, epoch=epoch)\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clipnorm)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        logits = outputs\n",
    "        probs = F.sigmoid(logits)\n",
    "        acc = multi_class_acc(probs, labels)\n",
    "        accuracy.update(acc.item())\n",
    "\n",
    "        if (iter + 1) % print_freq == 0 or iter == 0 or (iter + 1) == num_its:\n",
    "            print('\\r%5.1f   %5d    %0.6f   |  %0.4f  %0.4f  | ... ' % \\\n",
    "                  (epoch - 1 + (iter + 1) / num_its, iter + 1, lr, losses.avg, accuracy.avg), \\\n",
    "                  end='', flush=True)\n",
    "\n",
    "    return iter, losses.avg, accuracy.avg\n",
    "\n",
    "def validate(valid_loader, model, criterion, epoch, focal_loss, threshold=0.5):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    probs_list = []\n",
    "    labels_list = []\n",
    "    logits_list = []\n",
    "\n",
    "    end = time.time()\n",
    "    for it, iter_data in enumerate(valid_loader, 0):\n",
    "        images, labels, indices = iter_data\n",
    "        images = Variable(images.to(DEVICE))\n",
    "        labels = Variable(labels.to(DEVICE))\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels, epoch=epoch)\n",
    "\n",
    "        logits = outputs\n",
    "        probs = F.sigmoid(logits)\n",
    "        acc = multi_class_acc(probs, labels)\n",
    "\n",
    "        probs_list.append(probs.cpu().detach().numpy())\n",
    "        labels_list.append(labels.cpu().detach().numpy())\n",
    "        logits_list.append(logits.cpu().detach().numpy())\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        accuracy.update(acc.item())\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    probs = np.vstack(probs_list)\n",
    "    y_true = np.vstack(labels_list)\n",
    "    logits = np.vstack(logits_list)\n",
    "    valid_focal_loss = focal_loss.forward(torch.from_numpy(logits), torch.from_numpy(y_true))\n",
    "\n",
    "    y_pred = probs > threshold\n",
    "    kaggle_score = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return losses.avg, accuracy.avg, valid_focal_loss, kaggle_score\n",
    "\n",
    "def save_model(model, is_best, model_out_dir, optimizer=None, epoch=None, best_epoch=None, best_focal=None):\n",
    "    if type(model) == DataParallel:\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    for key in state_dict.keys():\n",
    "        state_dict[key] = state_dict[key].cpu()\n",
    "\n",
    "    model_fpath = opj(model_out_dir, '%03d.pth' % epoch)\n",
    "    torch.save({\n",
    "        'save_dir': model_out_dir,\n",
    "        'state_dict': state_dict,\n",
    "        'best_epoch': best_epoch,\n",
    "        'epoch': epoch,\n",
    "        'best_score': best_focal,\n",
    "    }, model_fpath)\n",
    "\n",
    "    optim_fpath = opj(model_out_dir, '%03d_optim.pth' % epoch)\n",
    "    if optimizer is not None:\n",
    "        torch.save({\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, optim_fpath)\n",
    "\n",
    "    if is_best:\n",
    "        best_model_fpath = opj(model_out_dir, 'final.pth')\n",
    "        shutil.copyfile(model_fpath, best_model_fpath)\n",
    "        if optimizer is not None:\n",
    "            best_optim_fpath = opj(model_out_dir, 'final_optim.pth')\n",
    "            shutil.copyfile(optim_fpath, best_optim_fpath)\n",
    "\n",
    "def multi_class_acc(preds, targs, th=0.5):\n",
    "    preds = (preds > th).int()\n",
    "    targs = targs.int()\n",
    "    return (preds == targs).float().mean()\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_results = Path('results')\n",
    "dir_results.mkdir(exist_ok=True, parents=True)\n",
    "out_dir = Path('external_crop256_focal_slov_hardlog_class_densenet121_dropout_i384_aug2_5folds')\n",
    "\n",
    "gpu_id = '0' # '0,1,2,3'\n",
    "arch = 'class_densenet121_dropout'\n",
    "num_classes = len(LABEL_NAME_LIST)\n",
    "scheduler = 'Adam55'\n",
    "epochs = 2 #55\n",
    "sz_img = 384\n",
    "crop_size = 64 #512\n",
    "batch_size = 16\n",
    "split_name = 'random_folds5'\n",
    "fold = 0\n",
    "workers = 0 # 3\n",
    "pin_memory = False  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Creating directory if it does not exist:\n",
      ">> 'results/models/external_crop256_focal_slov_hardlog_class_densenet121_dropout_i384_aug2_5folds/fold0'\n",
      ">> Using pre-trained model.\n",
      "** start training here! **\n",
      "\n",
      "epoch    iter      rate     |  train_loss/acc  |    valid_loss/acc/focal/kaggle     |best_epoch/best_focal|  min \n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "  1.0      37    0.000300   |  3.8598  0.5226  |    3.2634  0.5594 4.3016 0.1509    |     1.0    4.3016   | 1.2 min \n",
      "  2.0      37    0.000300   |  3.4374  0.5667  |    2.3335  0.6856 2.8685 0.2031    |     2.0    2.8685   | 1.1 min \n"
     ]
    }
   ],
   "source": [
    "main_training(dir_data, dir_mdata, dir_results, out_dir, \n",
    "     gpu_id=gpu_id, arch=arch, scheduler=scheduler,\n",
    "     epochs=epochs, img_size=sz_img, crop_size=crop_size,\n",
    "     batch_size=batch_size, split_name=split_name, fold=fold,\n",
    "     workers=workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
