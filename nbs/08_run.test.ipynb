{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp run.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../../HPA-competition-solutions/bestfitting/src/run/test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on collie.local\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from kgl_humanprotein.config.config import *\n",
    "from kgl_humanprotein.utils.common_util import *\n",
    "from kgl_humanprotein.networks.imageclsnet import init_network\n",
    "from kgl_humanprotein.datasets.protein_dataset import ProteinDataset\n",
    "from kgl_humanprotein.utils.augment_util import *\n",
    "from kgl_humanprotein.utils.log_util import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python test.py \\\n",
    "#             --out_dir external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds \\\n",
    "#             --gpu_id 0 --arch class_densenet121_dropout \\\n",
    "#             --img_size 768 --crop_size 512 --seeds 0,1,2,3 --batch_size 12 --fold 0 \\\n",
    "#             --augment\n",
    "\n",
    "python test.py \\\n",
    "            --out_dir external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds \\\n",
    "            --gpu_id 0 --arch class_densenet121_dropout \\\n",
    "            --img_size 768 --crop_size 512 --seeds 0,1,2,3 --batch_size 12 --fold 0 \\\n",
    "            --augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "datasets_names = ['test', 'val']\n",
    "split_names = ['random_ext_folds5', 'random_ext_noleak_clean_folds5']\n",
    "augment_list = ['default', 'flipud', 'fliplr','transpose', 'flipud_lr',\n",
    "                'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-df3c37c97b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimageclsnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "def main(outdir, gpu_id='0', arch='class_densenet121_dropout', \n",
    "         num_classes=28, in_channels=4, img_size=768, crop_size=512, \n",
    "         batch_size=32, workers=3, fold=0, augment='default', seed=100,  \n",
    "         seeds=None, dataset='test', split_name='random_ext_folds5',\n",
    "         predict_epoch=None):  \n",
    "    '''\n",
    "    PyTorch Protein Classification\n",
    "    \n",
    "    Args:\n",
    "        outdir (str): Destination where predicted result should be saved.\n",
    "        gpu_id (str): GPU id used for predicting. Default: ``'0'``\n",
    "        arch (str): Model architecture. Default: ``'class_densenet121_dropout)'``\n",
    "        num_classes (int): Number of classes. Default: 28\n",
    "        in_channels (int): In channels. Default: 4\n",
    "        img_size (int):  Image size. Default: 768\n",
    "        crop_size (int): Crop size. Default: 512\n",
    "        batch_size (int): Train mini-batch size. Default: 32\n",
    "        workers (int): Number of data loading workers. Default: 3\n",
    "        fold (int): Index of fold. Default: 0\n",
    "        augment (str):  Test augmentation Default: ``'default'``\n",
    "        seed (int):  Random seed. Default: 100\n",
    "        seeds (str): Predict seed. Default: None\n",
    "        dataset (str, optional): ``'test'``, or ``'val'``. Default: ``'test'``\n",
    "        split_name (str, optional): ``'random_ext_folds5'``, or\n",
    "            ``'random_ext_noleak_clean_folds5'``. Default: 'random_ext_folds5'\n",
    "        predict_epoch (int): Number epoch to predict. Default: None\n",
    "    '''\n",
    "    if dataset not in datasets_names:\n",
    "        print(f'`dataset` needs to be one of {datasets_names}.')\n",
    "        raise\n",
    "\n",
    "    if split_name not in split_names:\n",
    "        print(f'`split_name` must be one of {split_names}.')\n",
    "        raise\n",
    "\n",
    "    log_out_dir = opj(RESULT_DIR, 'logs', args.out_dir, 'fold%d' % args.fold)\n",
    "    if not ope(log_out_dir):\n",
    "        os.makedirs(log_out_dir)\n",
    "    log = Logger()\n",
    "    log.open(opj(log_out_dir, 'log.submit.txt'), mode='a')\n",
    "\n",
    "    args.predict_epoch = 'final' if args.predict_epoch is None else '%03d' % args.predict_epoch\n",
    "    network_path = opj(RESULT_DIR, 'models', args.out_dir, 'fold%d' % args.fold, '%s.pth' % args.predict_epoch)\n",
    "\n",
    "    submit_out_dir = opj(RESULT_DIR, 'submissions', args.out_dir, 'fold%d' % args.fold, 'epoch_%s' % args.predict_epoch)\n",
    "    log.write(\">> Creating directory if it does not exist:\\n>> '{}'\\n\".format(submit_out_dir))\n",
    "    if not ope(submit_out_dir):\n",
    "        os.makedirs(submit_out_dir)\n",
    "\n",
    "    # setting up the visible GPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "    args.augment = args.augment.split(',')\n",
    "    for augment in args.augment:\n",
    "        if augment not in augment_list:\n",
    "            raise ValueError('Unsupported or unknown test augmentation: {}!'.format(augment))\n",
    "\n",
    "    model_params = {}\n",
    "    model_params['architecture'] = args.arch\n",
    "    model_params['num_classes'] = args.num_classes\n",
    "    model_params['in_channels'] = args.in_channels\n",
    "    model = init_network(model_params)\n",
    "\n",
    "    log.write(\">> Loading network:\\n>>>> '{}'\\n\".format(network_path))\n",
    "    checkpoint = torch.load(network_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    log.write(\">>>> loaded network:\\n>>>> epoch {}\\n\".format(checkpoint['epoch']))\n",
    "\n",
    "    # moving network to gpu and eval mode\n",
    "    model = DataParallel(model)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    # Data loading code\n",
    "    dataset = args.dataset\n",
    "    if dataset == 'test':\n",
    "        test_split_file = opj(DATA_DIR, 'split', 'test_11702.csv')\n",
    "    elif dataset == 'val':\n",
    "        test_split_file = opj(DATA_DIR, 'split', args.split_name, 'random_valid_cv%d.csv' % args.fold)\n",
    "    else:\n",
    "        raise ValueError('Unsupported or unknown dataset: {}!'.format(dataset))\n",
    "    test_dataset = ProteinDataset(\n",
    "        test_split_file,\n",
    "        img_size=args.img_size,\n",
    "        is_trainset=(dataset != 'test'),\n",
    "        return_label=False,\n",
    "        in_channels=args.in_channels,\n",
    "        transform=None,\n",
    "        crop_size=args.crop_size,\n",
    "        random_crop=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    seeds = [args.seed] if args.seeds is None else [int(i) for i in args.seeds.split(',')]\n",
    "    for seed in seeds:\n",
    "        test_dataset.random_crop = (seed != 0)\n",
    "        for augment in args.augment:\n",
    "            test_loader.dataset.transform = eval('augment_%s' % augment)\n",
    "            if args.crop_size > 0:\n",
    "                sub_submit_out_dir = opj(submit_out_dir, '%s_seed%d' % (augment, seed))\n",
    "            else:\n",
    "                sub_submit_out_dir = opj(submit_out_dir, augment)\n",
    "            if not ope(sub_submit_out_dir):\n",
    "                os.makedirs(sub_submit_out_dir)\n",
    "            with torch.no_grad():\n",
    "                predict(test_loader, model, sub_submit_out_dir, dataset)\n",
    "\n",
    "def predict(test_loader, model, submit_out_dir, dataset):\n",
    "    all_probs = []\n",
    "    img_ids = np.array(test_loader.dataset.img_ids)\n",
    "    for it, iter_data in tqdm(enumerate(test_loader, 0), total=len(test_loader)):\n",
    "        images, indices = iter_data\n",
    "        images = Variable(images.cuda(), volatile=True)\n",
    "        outputs = model(images)\n",
    "        logits = outputs\n",
    "\n",
    "        probs = F.sigmoid(logits).data\n",
    "        all_probs += probs.cpu().numpy().tolist()\n",
    "    img_ids = img_ids[:len(all_probs)]\n",
    "    all_probs = np.array(all_probs).reshape(len(img_ids), -1)\n",
    "\n",
    "    np.save(opj(submit_out_dir, 'prob_%s.npy' % dataset), all_probs)\n",
    "\n",
    "    result_df = prob_to_result(all_probs, img_ids)\n",
    "    result_df.to_csv(opj(submit_out_dir, 'results_%s.csv.gz' % dataset), index=False, compression='gzip')\n",
    "\n",
    "def prob_to_result(probs, img_ids, th=0.5):\n",
    "    probs = probs.copy()\n",
    "    probs[np.arange(len(probs)), np.argmax(probs, axis=1)] = 1\n",
    "\n",
    "    pred_list = []\n",
    "    for line in probs:\n",
    "        s = ' '.join(list([str(i) for i in np.nonzero(line > th)[0]]))\n",
    "        pred_list.append(s)\n",
    "    result_df = pd.DataFrame({ID: img_ids, PREDICTED: pred_list})\n",
    "    return result_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('%s: calling main function ... \\n' % os.path.basename(__file__))\n",
    "    main()\n",
    "    print('\\nsuccess!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
